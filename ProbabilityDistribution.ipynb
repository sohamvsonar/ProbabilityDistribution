{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units', 'contains', 'no', 'wit']\n",
      "['as', 'of', 'that', 'day', ',', 'the', 'new', 'constitution', 'heralding', 'the']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from csv import QUOTE_NONE\n",
    "\n",
    "# Read the SST data\n",
    "datain_sst = pd.read_csv(\"D:\\ProbabilityDistribution\\SST-2/train.tsv\",delimiter=\"\\t\")\n",
    "datain_sst_low = datain_sst['sentence'].str.lower()\n",
    "\n",
    "#Read the qnli data\n",
    "datain_qnli = pd.read_csv(r\"D:\\ProbabilityDistribution\\QNLI/dev.tsv\",delimiter=\"\\t\",quoting=QUOTE_NONE)\n",
    "datain_qnli_low= datain_qnli['sentence'].str.lower()\n",
    "\n",
    "sst_token=[]\n",
    "\n",
    "#Tokenize sentences from SST dataset\n",
    "for i in datain_sst_low:\n",
    "    token = word_tokenize(i)\n",
    "    sst_token.extend(token)\n",
    "\n",
    "qnli_token=[]\n",
    "\n",
    "#Tokenize sentences from qnli dataset\n",
    "for i in datain_qnli_low:\n",
    "    token = word_tokenize(i)\n",
    "    qnli_token.extend(token)\n",
    "#print first 10 from each dataset\n",
    "print(sst_token[:10])\n",
    "print(qnli_token[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability distribution of sst dataset is '0.9999999999998211'\n",
      "The probability distribution of qnli dataset is '0.9999999999998687'\n"
     ]
    }
   ],
   "source": [
    "#function to find probability distribution of a token list \n",
    "def probability_distribution(token_list):\n",
    "    count={}\n",
    "\n",
    "    #count occurence of each token\n",
    "    for token in token_list:    \n",
    "        if token in count:\n",
    "            count[token] += 1\n",
    "        else:\n",
    "            count[token]=1\n",
    "    \n",
    "    token_length= len(token_list)\n",
    "    probability_d={}\n",
    "    \n",
    "    #Calculate probability of each token\n",
    "    for i, j in count.items():\n",
    "        probability = j / token_length\n",
    "        probability_d[i] = probability\n",
    "\n",
    "    return probability_d\n",
    "\n",
    "\n",
    "#print the probabilities for both datasets\n",
    "sst_probability_d= probability_distribution(sst_token)\n",
    "sst_probability= sum(sst_probability_d.values())\n",
    "print(f\"The probability distribution of sst dataset is '{sst_probability}'\")\n",
    "qnli_probability_d= probability_distribution(qnli_token)\n",
    "qnli_probability= sum(qnli_probability_d.values())\n",
    "print(f\"The probability distribution of qnli dataset is '{qnli_probability}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word level entropy of sst dataset is '10.079162530566823'\n",
      " word level entropy of qnli dataset is '10.037404792966129'\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# A function to find the entropy of probability distribution\n",
    "def find_entropy(probability_distribution):\n",
    "    entropy = 0\n",
    "\n",
    "    for i in probability_distribution.values():\n",
    "        if i > 0:\n",
    "            entropy -= i * math.log2(i)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "#print word-level entropy for sst and qnli datasets\n",
    "print(f\" word level entropy of sst dataset is '{find_entropy(sst_probability_d)}'\")\n",
    "print(f\" word level entropy of qnli dataset is '{find_entropy(qnli_probability_d)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl divergence for sst dataset w.r.t qnli dataset is '0.829548217709137'\n",
      "kl divergence for qnli dataset w.r.t sst dataset is '0.7374204121557258'\n",
      "As both the values are different KL divergence is not symmetric\n"
     ]
    }
   ],
   "source": [
    "# A function to find the kl divergence between the two probability distributions\n",
    "def find_kl_divergence(pd_1, pd_2):\n",
    "    kl_divergence = 0.0\n",
    "    for i in pd_1:\n",
    "        if pd_1[i] > 0 and i in pd_2:\n",
    "            kl_divergence += pd_1[i] * math.log2(pd_1[i] / pd_2[i])\n",
    "    return kl_divergence\n",
    "\n",
    "#print KL divergence results and check for symmetry\n",
    "print(f\"kl divergence for sst dataset w.r.t qnli dataset is '{find_kl_divergence(sst_probability_d, qnli_probability_d)}'\")\n",
    "print(f\"kl divergence for qnli dataset w.r.t sst dataset is '{find_kl_divergence(qnli_probability_d, sst_probability_d)}'\")\n",
    "\n",
    "print(\"As both the values are different KL divergence is not symmetric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006698149655695167\n",
      "0.009244245596792456\n"
     ]
    }
   ],
   "source": [
    "# A function to find the entropy rate of a message given a probability distribution\n",
    "def find_entropy_rate(message, probability_distribution):\n",
    "    \n",
    "    tokens = word_tokenize(message.lower())      #Tokenize and convert to lowercase\n",
    "\n",
    "    total_entropy = 0 \n",
    "    length_tokens = len(tokens)\n",
    "\n",
    "    for i in tokens:\n",
    "        if i in probability_distribution:\n",
    "            token_probability = probability_distribution[i]\n",
    "        else:\n",
    "            token_probability = 1e-10            #small epsilon for missing tokens\n",
    "\n",
    "        total_entropy = -token_probability * math.log2(token_probability)\n",
    "        total_entropy+=total_entropy\n",
    "\n",
    "    entropy_rate = total_entropy / length_tokens\n",
    "\n",
    "    return entropy_rate\n",
    "\n",
    "#Example of a movie review\n",
    "movie_review = '''With its unique visual style and a story that captures the essence of the franchise's appeal,\n",
    " Teenage Mutant Ninja Turtles: Mutant Mayhem is an animated treat for the whole family.'''\n",
    "print(find_entropy_rate(movie_review, sst_probability_d))\n",
    "print(find_entropy_rate(movie_review, qnli_probability_d))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
